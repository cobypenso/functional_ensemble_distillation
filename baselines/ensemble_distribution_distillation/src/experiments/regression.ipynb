{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10811,
     "status": "ok",
     "timestamp": 1558128143256,
     "user_tz": -60
    },
    "id": "xAYPKSFrG8AF",
    "outputId": "cf3df00b-2fcc-48ba-bd75-af624f50aa20"
   },
   "outputs": [],
   "source": [
    "\"\"\"Learning \"logit\" distribution for UCI data\"\"\"\n",
    "import os\n",
    "CODE_DIR = <Path to repo 'code' dir>\n",
    "os.chdir(CODE_DIR)\n",
    "import logging\n",
    "import zipfile\n",
    "from copy import copy, deepcopy\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.sgd import SGD\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.model_selection import KFold\n",
    "from src.dataloaders.uci import uci_base, wine, bost\n",
    "from src import metrics\n",
    "from src import utils\n",
    "from src.ensemble import simple_regressor, ensemble\n",
    "from src.distilled import gauss_logits, norm_inv_wish\n",
    "from src import loss as custom_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = True\n",
    "args.log_dir = Path(\"./logs\")\n",
    "args.log_level = logging.WARNING\n",
    "args.retrain = True\n",
    "args.num_ensemble_members=1\n",
    "args.num_epochs=1\n",
    "args.lr = 0.01\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"uci_wine\"\n",
    "\n",
    "log_file = Path(\"{}_{}.log\".format(\n",
    "    EXPERIMENT_NAME,\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "utils.setup_logger(log_path=Path.cwd() / args.log_dir / log_file,\n",
    "                   log_level=args.log_level)\n",
    "\n",
    "# General constructs\n",
    "train_metrics = list()\n",
    "test_metrics = list()\n",
    "\n",
    "rmse = metrics.Metric(name=\"RMSE\", function=metrics.root_mean_squared_error)\n",
    "train_metrics.append(deepcopy(rmse))\n",
    "test_metrics.append(rmse)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1S5kt0omQ-N"
   },
   "outputs": [],
   "source": [
    "def create_ensemble(num_ensemble_members,\n",
    "                    input_dim,\n",
    "                    num_hidden,\n",
    "                    lr,\n",
    "                    ensemble_output_size):\n",
    "    prob_ensemble = ensemble.Ensemble(ensemble_output_size)\n",
    "    for _ in range(num_ensemble_members):\n",
    "        network = simple_regressor.Model(layer_sizes=[input_dim,\n",
    "                                                      num_hidden,\n",
    "                                                      ensemble_output_size],\n",
    "                                 device=device,\n",
    "                                 variance_transform=utils.positive_linear_asymptote(1e-6),\n",
    "                                 loss_function=custom_loss.gaussian_neg_log_likelihood_1d)\n",
    "        network.optimizer = torch.optim.Adam(network.parameters(),\n",
    "                                    lr=lr)\n",
    "        prob_ensemble.add_member(network)\n",
    "    return prob_ensemble\n",
    "\n",
    "def mean_and_std_from_list(samples):\n",
    "    \"\"\"Calculate mean and std from np-array compatible list\"\"\"\n",
    "    array = np.array(samples)\n",
    "    return array.mean(), array.std()\n",
    "\n",
    "def mean_and_std_from_metric(metric, rescale=1):\n",
    "    \"\"\"Calculate mean and std from np-array compatible list\"\"\"\n",
    "    return metric.mean() * rescale, metric.std() * rescale\n",
    "\n",
    "def test_distilled(distilled, x, y_true, device, scale):\n",
    "    with torch.no_grad():\n",
    "        num_samples = len(y_true)\n",
    "        x = torch.tensor(x).float().to(device)\n",
    "        z_mean, z_var = distilled.forward(x);\n",
    "        mu_dist = z_mean[:, 0].reshape(y_true.shape)\n",
    "        ale_dist = torch.log(1 + torch.exp(z_mean[:, 1]))\n",
    "        epi_dist = z_var[:, 1]\n",
    "        tot_uncert = ale_dist + epi_dist\n",
    "        y_true = torch.tensor(y_true,\n",
    "                              device=device,\n",
    "                              dtype=torch.float).reshape((num_samples, 1, 1))\n",
    "\n",
    "        rmse, nll, ause = common_test(y_true=y_true,\n",
    "                                      mu=mu_dist,\n",
    "                                      sigma_sq=tot_uncert,\n",
    "                                      uncert=tot_uncert)\n",
    "        rmse *= scale\n",
    "        nll += np.log(scale)\n",
    "        \n",
    "    return rmse.item(), nll.item(), ause.item()\n",
    "\n",
    "    \n",
    "def test_ensemble(prob_ensemble, x, y_true, device, scale):\n",
    "    with torch.no_grad():\n",
    "        num_samples = len(y_true)\n",
    "        output = prob_ensemble.predict(torch.tensor(x, device=device, dtype=torch.float))\n",
    "        mu_ens, sigma_sq_ens = output[:, :, 0],  output[:, :, 1]\n",
    "        mean_mu, tot_uncert = utils.gaussian_mixture_moments(mu_ens, sigma_sq_ens)\n",
    "        mean_mu = mean_mu.reshape((num_samples, 1)).to(device)\n",
    "        tot_uncert = tot_uncert.reshape((num_samples, 1)).to(device)\n",
    "        y_true = torch.tensor(y_true,\n",
    "                              device=device,\n",
    "                              dtype=torch.float).reshape((num_samples, 1, 1))\n",
    "\n",
    "        rmse, nll, ause = common_test(y_true, mean_mu, tot_uncert, tot_uncert)\n",
    "        rmse *= scale\n",
    "        nll += np.log(scale)\n",
    "    \n",
    "    return rmse.item(), nll.item(), ause.item()\n",
    "\n",
    "def common_test(y_true, mu, sigma_sq, uncert, num_partitions=10):\n",
    "    rmse = metrics.root_mean_squared_error(predictions=mu,\n",
    "                                           targets=y_true)\n",
    "    nll = custom_loss.gaussian_neg_log_likelihood_1d((mu, sigma_sq),\n",
    "                                                    y_true)\n",
    "    num_samples = len(y_true)\n",
    "    ause = ause_mix = utils.ause(y_true=y_true.reshape((num_samples, 1)),\n",
    "           y_pred=mu,\n",
    "           uncert_meas=uncert,\n",
    "           num_partitions=num_partitions)\n",
    "    return rmse, nll, ause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AmcpT5DDO2d"
   },
   "outputs": [],
   "source": [
    "def train_ensemble(data,\n",
    "                     num_ensemble_members,\n",
    "                     num_epochs,\n",
    "                     num_units,\n",
    "                     n_splits,\n",
    "                     learn_rate,\n",
    "                     weight_decay,\n",
    "                     train_metrics,\n",
    "                     test_metrics,\n",
    "                     batch_size):\n",
    "\n",
    "    ens_rmses = list()\n",
    "    ens_nlls = list()\n",
    "    ens_auses = list()\n",
    "    \n",
    "    dist_rmses = list()\n",
    "    dist_nlls = list()\n",
    "    dist_auses = list()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "    \n",
    "    hidden_size = 50\n",
    "    distilled_output_size = 4\n",
    "    layer_sizes = [in_dim, hidden_size, hidden_size, distilled_output_size]\n",
    "\n",
    "    for j, idx in enumerate(kf.split(data)):\n",
    "        train_index, test_index = idx\n",
    "        print(\"Fold: {}\".format(j))\n",
    "        for metric in train_metrics:\n",
    "            metric.reset()        \n",
    "\n",
    "        for metric in test_metrics:\n",
    "            metric.reset()\n",
    "\n",
    "        prob_ensemble = create_ensemble(num_ensemble_members=num_ensemble_members,\n",
    "                                        input_dim=in_dim,\n",
    "                                        num_hidden=num_units,\n",
    "                                        lr=learn_rate,\n",
    "                                        ensemble_output_size=2)\n",
    "        prob_ensemble.add_metrics(train_metrics)\n",
    "        \n",
    "        distilled_model = gauss_logits.Model(\n",
    "            layer_sizes=layer_sizes,\n",
    "            teacher=prob_ensemble,\n",
    "            variance_transform=utils.positive_linear_asymptote(0.001),\n",
    "            device=device,\n",
    "            learning_rate=args.lr)\n",
    "\n",
    "\n",
    "        #x_train, y_train, x_test, y_test = data.create_train_val_split(0.9)\n",
    "        \n",
    "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
    "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
    "\n",
    "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "        x_train = (x_train - x_means) / x_stds\n",
    "        y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "        x_test = (x_test - x_means) / x_stds\n",
    "        y_test = (y_test - y_means) / y_stds\n",
    "\n",
    "        data_std = y_stds[0]\n",
    "        if batch_size is None:\n",
    "            batch_size = x_train.shape[0]\n",
    "            \n",
    "        trainloader = uci_base.uci_dataloader(x_train, y_train, batch_size)\n",
    "        unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "\n",
    "\n",
    "        train_loss = prob_ensemble.train(train_loader=trainloader,\n",
    "                            num_epochs=num_epochs)\n",
    "        try:\n",
    "            distilled_model.train(unlabelled_loader, 30)\n",
    "        except (ValueError, RuntimeError):\n",
    "            print(\"NaN\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        rmse_ens, nll_ens, ause_ens = test_ensemble(prob_ensemble=prob_ensemble,\n",
    "                                                    x=x_test,\n",
    "                                                    y_true=y_test,\n",
    "                                                    device=device,\n",
    "                                                    scale=data_std)\n",
    "        rmse_dist, nll_dist, ause_dist = test_distilled(distilled_model,\n",
    "                                                     x=x_test,\n",
    "                                                     y_true=y_test,\n",
    "                                                     device=device,\n",
    "                                                     scale=data_std)\n",
    "            \n",
    "\n",
    "        ens_rmses.append(rmse_ens)\n",
    "        ens_nlls.append(nll_ens)\n",
    "        ens_auses.append(ause_ens)\n",
    "        \n",
    "        dist_rmses.append(rmse_dist)\n",
    "        dist_nlls.append(nll_dist)\n",
    "        dist_auses.append(ause_dist)\n",
    "        print(\"rmse: {}\\t nll: {}, ause: {}\".format(rmse_ens, nll_ens, ause_ens))\n",
    "        print(\"rmse: {}\\t nll: {}, ause: {}\".format(rmse_dist, nll_dist, ause_dist))\n",
    "\n",
    "    print(\"Test RMSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_rmses)))\n",
    "    print(\"Test NLL\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_nlls)))\n",
    "    print(\"Test AUSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_auses)))\n",
    "    print(\"Test RMSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_rmses)))\n",
    "    print(\"Test NLL\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_nlls)))\n",
    "    print(\"Test AUSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_auses)))\n",
    "    \n",
    "    return (ens_rmses, ens_nlls, ens_auses), (dist_rmses, dist_nlls, dist_auses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4542,
     "status": "ok",
     "timestamp": 1558130529014,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "KOqgIBXcCegJ",
    "outputId": "21e7e896-5a30-4692-ff6d-72182f4b83b4"
   },
   "outputs": [],
   "source": [
    "wine_data = wine.WineData(\"data/uci/wine/winequality-red.csv\")\n",
    "result  = train_ensemble(data=wine_data.data,\n",
    "                       num_ensemble_members=10,\n",
    "                       num_epochs=40,\n",
    "                       num_units=50,\n",
    "                       n_splits=5,\n",
    "                       learn_rate=1e-1,\n",
    "                       weight_decay=0.0, #1e-1/len(data)**0.5,\n",
    "                       train_metrics=train_metrics,\n",
    "                       test_metrics=test_metrics,\n",
    "                       batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, _, _ = wine_data.create_train_val_split(1)\n",
    "\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_tensor = torch.tensor(x_train).float().to(device)\n",
    "ens_output = prob_ensemble.predict(x_tensor)\n",
    "mu_ens = ens_output[:, :, 0]\n",
    "var_ens = ens_output[:, :, 1]\n",
    "mean_mu_ens = torch.mean(mu_ens, dim=1).reshape(y_train.shape).cpu().detach().numpy()\n",
    "\n",
    "ale_ens, epi_ens = metrics.uncertainty_separation_parametric(mu_ens, var_ens)\n",
    "ale_ens = ale_ens.detach().numpy()\n",
    "epi_ens = epi_ens.detach().numpy()\n",
    "\n",
    "z_mean, z_var = distilled_model.forward(x_tensor);\n",
    "z_mean = z_mean.cpu().detach()\n",
    "z_var = z_var.cpu().detach().numpy()\n",
    "mu_dist = z_mean[:, 0].reshape(y_train.shape).numpy()\n",
    "ale_dist = torch.log(1 + torch.exp(z_mean[:, 1])).numpy()\n",
    "epi_dist = z_var[:, 1]\n",
    "\n",
    "dist_spread = z_var.sum(1)\n",
    "window_size = 20\n",
    "uncert_ens = ale_ens + epi_ens\n",
    "fig, (ax_ens, ax_dist) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "\n",
    "uncert_dist = ale_dist + epi_dist\n",
    "\n",
    "num_partitions = 10\n",
    "\n",
    "utils.plot_sparsification_error(ax_ens,\n",
    "                 y_true=y_train,\n",
    "                 y_pred=mean_mu_ens,\n",
    "                 uncert_meas=uncert_ens,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Ensemble\")\n",
    "\n",
    "utils.plot_sparsification_error(ax_dist,\n",
    "                 y_true=y_train,\n",
    "                 y_pred=mu_dist,\n",
    "                 uncert_meas=uncert_dist,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Distilled\")\n",
    "\n",
    "ax_ens.set_ylabel(\"$SE$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "distilled_output_size = 4\n",
    "layer_sizes = [wine_data.input_dim, hidden_size, hidden_size, distilled_output_size]\n",
    "distilled_model = logits_probability_distribution.LogitsProbabilityDistribution(\n",
    "    layer_sizes=layer_sizes,\n",
    "    teacher=prob_ensemble,\n",
    "    variance_transform=utils.positive_linear_asymptote(),\n",
    "    device=device,\n",
    "    learning_rate=args.lr)\n",
    "\n",
    "unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "test_loader = uci_base.uci_dataloader(x_test, y_test, len(y_test))\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 30)\n",
    "\n",
    "x_train, y_train, x_test, y_test = wine_data.create_train_val_split(0.9)\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_test = (x_test - x_means) / x_stds\n",
    "y_test = (y_test - y_means) / y_stds\n",
    "\n",
    "unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "test_loader = uci_base.uci_dataloader(x_test, y_test, len(y_test))\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = wine_data.create_train_val_split(0.9)\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_test = (x_test - x_means) / x_stds\n",
    "y_test = (y_test - y_means) / y_stds\n",
    "test_nlls = list()\n",
    "test_rmses = list()\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test, device=device, dtype=torch.float)\n",
    "    y_test_tensor = torch.tensor(y_test,\n",
    "                                     device=device,\n",
    "                                     dtype=torch.float).reshape((len(y_test), 1, 1))\n",
    "    \n",
    "    mean_dist, var_dist = distilled_model.forward(x_test_tensor)\n",
    "    mu_dist = mean_dist[:, 0].unsqueeze(1)\n",
    "    sigma_sq_dist = distilled_model.variance_transform(mean_dist[:, 0].unsqueeze(1))\n",
    "    test_rmse = metrics.root_mean_squared_error(predictions=mu_dist,\n",
    "                                    targets=y_test_tensor) * data_std\n",
    "    test_nll = custom_loss.gaussian_neg_log_likelihood_1d((mu_dist, sigma_sq_dist),\n",
    "                                                    y_test_tensor) + np.log(data_std)\n",
    "\n",
    "test_nlls.append(test_nll.item())\n",
    "test_rmses.append(test_rmse.item())\n",
    "\n",
    "print(test_rmse)\n",
    "print(test_nll)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mc_dropout_heteroscedastic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
