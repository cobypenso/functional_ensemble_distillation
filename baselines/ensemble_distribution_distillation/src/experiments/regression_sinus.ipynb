{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from copy import deepcopy\n",
    "import os\n",
    "os.chdir(\"/home/jakob/doktor/projects/EnsembleUncertainty/code\")\n",
    "\"\"\"Learing \"logit\" distribution in regression example\"\"\"\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from src.dataloaders import gaussian_sinus, one_dim_regression\n",
    "import src.utils as utils\n",
    "from src.distilled import gauss_logits, gauss_mix, norm_inv_wish\n",
    "from src.ensemble import simple_regressor, ensemble\n",
    "\n",
    "import src.metrics as metrics\n",
    "import src.loss as custom_loss\n",
    "from src import utils\n",
    "from src.utils_dir import tikz as tikz_utils\n",
    "from src.utils_dir import experiments as experiment_utils\n",
    "from src.utils_dir import pytorch as torch_utils\n",
    "from src.utils_dir import plot as plot_utils\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"niw_distillation\"\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = True\n",
    "args.log_dir = Path(\"./logs\")\n",
    "\n",
    "args.log_level = logging.INFO\n",
    "args.retrain = True\n",
    "\n",
    "args.num_ensemble_members=50\n",
    "args.num_epochs=100\n",
    "args.lr = 0.01\n",
    "\n",
    "log_file = Path(\"{}_{}.log\".format(\n",
    "    EXPERIMENT_NAME,\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "utils.setup_logger(log_path=Path.cwd() / args.log_dir / log_file,\n",
    "                   log_level=args.log_level)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 15]\n",
    "\n",
    "tex_dir = Path(\"/home/jakob/doktor/projects/EnsembleUncertainty/paper/Paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(distilled_model, data):\n",
    "    test_loader = torch.utils.data.DataLoader(data,\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "    predictions = np.zeros((data.n_samples, distilled_model.output_size))\n",
    "    all_x = np.zeros((data.n_samples, 1))\n",
    "    all_y = np.zeros((data.n_samples, 1))\n",
    "\n",
    "    idx = 0\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "\n",
    "        predictions[idx * test_loader.batch_size:(idx + 1) * test_loader.batch_size, :, :] = \\\n",
    "            distilled_model.predict(inputs, t=None).data.numpy()\n",
    "\n",
    "        all_x[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = inputs\n",
    "        all_y[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = targets\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    plt.scatter(np.squeeze(all_x), np.squeeze(all_y), label=\"Data\", marker=\".\")\n",
    "\n",
    "    plt.errorbar(np.squeeze(all_x),\n",
    "                 predictions[:, 0],\n",
    "                 np.sqrt(predictions[:, 1]),\n",
    "                 label=\"Distilled model predictions\",\n",
    "                 marker=\".\",\n",
    "                 ls=\"none\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER.info(\"Args: {}\".format(args))\n",
    "device = utils.torch_settings(args.seed, args.gpu)\n",
    "LOGGER.info(\"Creating dataloader\")\n",
    "data = gaussian_sinus.GaussianSinus(range_=(-3, 3), store_file=Path(\"data/tmp_data\"))\n",
    "\n",
    "input_size = 1\n",
    "ensemble_output_size = 2\n",
    "layer_sizes = [1, 10, 10, ensemble_output_size] # Mean and variance\n",
    "args.num_ensemble_members = 2\n",
    "args.num_epochs=25\n",
    "args.lr = 0.001\n",
    "args.log_level = logging.INFO\n",
    "train_loader = torch.utils.data.DataLoader(data,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=True)\n",
    "\n",
    "prob_ensemble = ensemble.Ensemble(ensemble_output_size)\n",
    "for _ in range(args.num_ensemble_members):\n",
    "\n",
    "    model = simple_regressor.Model(layer_sizes=layer_sizes,\n",
    "                                 device=device,\n",
    "                                 variance_transform=utils.positive_linear_asymptote(),\n",
    "                                 loss_function=custom_loss.gaussian_nll_1d)\n",
    "    model.optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                         lr=args.lr)\n",
    "    #model.switch_active_network(\"mu\")\n",
    "    prob_ensemble.add_member(model)\n",
    "squared_error_metric = metrics.Metric(name=\"MSE\",\n",
    "                                      function=metrics.mean_squared_error)\n",
    "prob_ensemble.add_metrics([squared_error_metric])\n",
    "prob_ensemble.train(train_loader, args.num_epochs)\n",
    "\n",
    "ensemble_experiment = experiment_utils.experiment_info(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = gaussian_sinus.GaussianSinus(range_=(-5, 5),\n",
    "                                             store_file=Path(\"tmp_data\"))\n",
    "test_data = test_data_set.get_full_data(sorted_=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    N = len(test_data)\n",
    "    x = torch.tensor(test_data[:, 0]).float()\n",
    "    y = torch.tensor(test_data[:, 1]).float()\n",
    "    x = x.reshape((N, 1))\n",
    "    ens_output = prob_ensemble.predict(x).cpu()\n",
    "    x = x.reshape(y.size())\n",
    "    mu_ens, sigma_sq_ens = ens_output[:, :, 0],  ens_output[:, :, 1]\n",
    "    mean_mu_ens, sigma_sq = torch_utils.gaussian_mixture_moments(mu_ens, sigma_sq_ens)\n",
    "    ale_ens, epi_ens = metrics.uncertainty_separation_parametric(mu_ens, sigma_sq_ens)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "plot_utils.plot_uncert(ax, x, y, mean_mu=mean_mu_ens,every_nth=10, ale=ale_ens, epi=epi_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create distilled!\n",
    "hidden_size = 20\n",
    "distilled_output_size = ensemble_output_size * 2 + 2\n",
    "layer_sizes = [input_size, hidden_size, hidden_size, distilled_output_size]\n",
    "#distilled_model = gauss_logits.Model(\n",
    "#    layer_sizes=layer_sizes,\n",
    "#    variance_transform=utils.positive_linear_asymptote(),\n",
    "#    teacher=prob_ensemble,\n",
    "#    device=device,\n",
    "#    learning_rate=args.lr)\n",
    "args.lr = 0.0001\n",
    "distilled_model = norm_inv_wish.Model(\n",
    "    layer_sizes=layer_sizes,\n",
    "    target_dim=1,\n",
    "    variance_transform=utils.positive_linear_asymptote(1e-3),\n",
    "    teacher=prob_ensemble,\n",
    "    device=device,\n",
    "    learning_rate=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrain!\n",
    "\n",
    "lower = -3\n",
    "upper = 3\n",
    "unlabelled_data = gaussian_sinus.GaussianSinus(n_samples=100,\n",
    "                                               store_file=Path(\"data/temp_data_distilled\"),\n",
    "                                               train=False,\n",
    "                                               range_=(lower, upper))\n",
    "unlabelled_loader = torch.utils.data.DataLoader(unlabelled_data,\n",
    "                                                batch_size=8,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=0)\n",
    "\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(x)\n",
    "with torch.no_grad():\n",
    "    x = torch.tensor(test_data[:, 0]).float()\n",
    "    y = torch.tensor(test_data[:, 1]).float()\n",
    "    x = x.reshape((num_samples, 1))\n",
    "    ens_output = prob_ensemble.predict(x).cpu()\n",
    "    mu_dist, scale_dist, psi_dist, nu_dist = distilled_model.forward(x.to(device))\n",
    "    ale_dist = psi_dist / (nu_dist - distilled_model.target_dim) / scale_dist\n",
    "    x = x.reshape(y.size())\n",
    "    \n",
    "\n",
    "mu_ens, sigma_sq_ens = ens_output[:, :, 0],  ens_output[:, :, 1]\n",
    "mean_mu_ens, mean_sigma_sq_ens = torch_utils.gaussian_mixture_moments(mu_ens, sigma_sq_ens)\n",
    "#mean_mu_ens = torch.mean(mu_ens, dim=1)\n",
    "ale_ens, epi_ens = metrics.uncertainty_separation_parametric(mu_ens, sigma_sq_ens)\n",
    "\n",
    "# mu_dist = z_mean[:, 0]\n",
    "#ale_dist = torch.log( 1 + torch.exp(z_mean[:, 1]))\n",
    "# epi_dist = z_var[:, 1]\n",
    "_, true_mean, true_sigma = test_data_set.x_to_y_mapping(x)\n",
    "\n",
    "\n",
    "x = x.numpy()\n",
    "y = y.numpy()\n",
    "mean_mu_ens = mean_mu_ens.numpy()\n",
    "mu_dist = mu_dist.reshape(mean_mu_ens.shape).cpu().numpy()\n",
    "ale_ens = ale_ens.numpy()\n",
    "epi_ens = epi_ens.numpy()\n",
    "ale_dist = ale_dist.reshape(mean_mu_ens.shape).cpu().numpy()\n",
    "#epi_dist = epi_dist.numpy()\n",
    "\n",
    "#_, ax = plt.subplots()\n",
    "#plot_utils.plot_uncert(ax, x, y, mean_mu=true_mean, every_nth=10, ale=np.sqrt(true_sigma), epi=None)\n",
    "#tikzplotlib.save(\"fig/toy_example/data.tikz\")\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "plot_utils.plot_uncert(ax, x, y, mean_mu=mean_mu_ens,every_nth=10, ale=ale_ens, epi=epi_ens)\n",
    "#tikz_utils.save(filepath=\"fig/toy_example/ensemble.tikz\",\n",
    "#                comment=experiment_utils.experiment_info(model, args))\n",
    "#_, ax = plt.subplots()\n",
    "#plot_utils.plot_uncert(ax, x, y, mean_mu=mu_dist, every_nth=10, ale=ale_dist, epi=epi_dist)\n",
    "#tikz_utils.save(filepath=\"fig/toy_example/distilled.tikz\",\n",
    "#                comment=experiment_utils.experiment_info(distilled_model, args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.tensor(test_data[:, 0]).float()\n",
    "    y = torch.tensor(test_data[:, 1]).float()\n",
    "    x = x.reshape((len(x), 1))\n",
    "    output = prob_ensemble.predict(x)\n",
    "mu_ens, sigma_sq_ens = ens_output[:, :, 0],  ens_output[:, :, 1]\n",
    "mean_mu_ens, mean_sigma_sq_ens = utils.gaussian_mixture_moments(mu_ens, sigma_sq_ens)\n",
    "\n",
    "gauss_param = (mean_mu_ens.unsqueeze_(1), mean_sigma_sq_ens.unsqueeze_(1))\n",
    "mixture_targets = (mu_ens, sigma_sq_ens)\n",
    "custom_loss.kl_div_gauss_and_mixture_of_gauss(gauss_param, mixture_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create mixture!\n",
    "hidden_size = 10\n",
    "distilled_output_size = ensemble_output_size\n",
    "layer_sizes = [input_size, hidden_size, hidden_size, distilled_output_size]\n",
    "gauss_mix_distilled = gauss_mix.Model(layer_sizes=layer_sizes,\n",
    "                                      teacher=prob_ensemble,\n",
    "                                      loss_function=custom_loss.kl_div_gauss_and_mixture_of_gauss,\n",
    "                                      device=device)\n",
    "gauss_mix_distilled.optimizer = torch.optim.Adam(gauss_mix_distilled.parameters(),\n",
    "                                                 lr=args.lr)\n",
    "\n",
    "# Retrain!\n",
    "lower = -5\n",
    "upper = 5\n",
    "unlabelled_data = gaussian_sinus.GaussianSinus(\n",
    "    store_file=Path(\"None\"), train=False, range_=(lower, upper))\n",
    "unlabelled_loader = torch.utils.data.DataLoader(unlabelled_data,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "\n",
    "gauss_mix_distilled.train(unlabelled_loader, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = torch.tensor(test_data[:, 0]).float()\n",
    "    y = torch.tensor(test_data[:, 1]).float()\n",
    "    x = x.reshape((len(x), 1))\n",
    "    mu_mix, sigma_sq_mix = gauss_mix_distilled.forward(x.to(device))\n",
    "    mu_mix, sigma_sq_mix = mu_mix.cpu().reshape(y.size()), sigma_sq_mix.cpu().reshape(y.size())\n",
    "    x = x.reshape(y.size())\n",
    "    \n",
    "\n",
    "_, ax = plt.subplots()\n",
    "utils.plot_uncert(ax, x, y, every_nth=10, mean_mu=mu_mix, ale=None, epi=sigma_sq_mix)\n",
    "tikzplotlib.save(\"fig/toy_example/direct_distilled_mu_uncert.tikz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = (y - mu_dist)**2\n",
    "fig, (ax_ens, ax_dist, ax_mix) = plt.subplots(1, 3, sharey=True)\n",
    "\n",
    "window_size = 20\n",
    "uncert_ens = ale_ens + epi_ens\n",
    "uncert_dist = ale_dist + epi_dist\n",
    "uncert_mix = sigma_sq_mix\n",
    "\n",
    "num_partitions = 10\n",
    "# AUSE\n",
    "utils.plot_sparsification_error(ax_ens,\n",
    "                 y_true=y,\n",
    "                 y_pred=mean_mu_ens,\n",
    "                 uncert_meas=uncert_ens,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Ensemble\")\n",
    "\n",
    "ause_ens = utils.ause(y_true=y,\n",
    "           y_pred=mean_mu_ens,\n",
    "           uncert_meas=uncert_ens,\n",
    "           num_partitions=num_partitions)\n",
    "\n",
    "utils.plot_sparsification_error(ax_dist,\n",
    "                 y_true=y,\n",
    "                 y_pred=mu_dist,\n",
    "                 uncert_meas=uncert_dist,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Distilled\")\n",
    "ause_dist = utils.ause(y_true=y,\n",
    "           y_pred=mu_dist,\n",
    "           uncert_meas=uncert_dist,\n",
    "           num_partitions=num_partitions)\n",
    "\n",
    "utils.plot_sparsification_error(ax_mix,\n",
    "                 y_true=y,\n",
    "                 y_pred=mu_mix,\n",
    "                 uncert_meas=uncert_mix,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Direct distilled\")\n",
    "ause_mix = utils.ause(y_true=y,\n",
    "           y_pred=mu_mix,\n",
    "           uncert_meas=uncert_mix,\n",
    "           num_partitions=num_partitions)\n",
    "#fig.set_xlabel(\"$x^{(i)}$\")\n",
    "ax_ens.set_ylabel(\"$SE$\")\n",
    "\n",
    "reg_dir = Path(\"Experiments/Regression/fig/\")\n",
    "#tikzplotlib.save(tex_dir/reg_dir/\"toy_example/sparse_err.tikz\")\n",
    "\n",
    "\n",
    "print(ause_ens.item(), ause_dist.item(), ause_mix.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dir = Path(\"Experiments/Regression/data/\")\n",
    "utils.csv_result(result, file=tex_dir/reg_dir/(EXPERIMENT_NAME + \".csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils_dir.pytorch as torch_utils\n",
    "\n",
    "model = prob_ensemble.members[0]\n",
    "experiment_utils.experiment_info(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#B, N, D = 2, 1, 1\n",
    "#target = torch.tensor([[1.0], [0.75]],\n",
    "#                      dtype=torch.float).reshape(B, N, D)\n",
    "#mean = torch.tensor([[0.5], [0.25]], dtype=torch.float)\n",
    "#var = torch.tensor([[10.0], [5.0]], dtype=torch.float)\n",
    "model = prob_ensemble.members[0]\n",
    "\n",
    "inputs, targets = next(iter(train_loader))\n",
    "inputs, targets =inputs.float().to(device), targets.float().to(device)\n",
    "targets = targets.reshape((targets.size(0), 1, 1))\n",
    "parameters = model.transform_logits(model.forward(inputs))\n",
    "old = custom_loss.gaussian_nll_1d(parameters, targets)\n",
    "new = custom_loss.new_gaussian_nll_1d(parameters, targets)\n",
    "print(old)\n",
    "print(new)\n",
    "\n",
    "#targets.mean(dim=1, keepdim=True) - targets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
